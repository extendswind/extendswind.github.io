<!DOCTYPE html>
<html lang='en'><head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='Spark提供了HDFS上一般的文件文件读取接口 sc.textFile()，但在某些情况下HDFS中需要存储自定义格式的文件，需要更加灵活的读取方式。
使用KeyValueTextInputFormat Hadoop的MapReduce框架下提供了一些InputFormat的实现，其中MapReduce2的接口(org.apache.hadoop.mapreduce下)与先前MapReduce1(org.apache.hadoop.mapred下)有区别，对应于newAPIHadoopFile函数。
使用KeyValueTextInputFormat的文件读取如下
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat import org.apache.hadoop.io.Text val hFile = sc.newAPIHadoopFile(&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;, classOf[KeyValueTextInputFormat], classOf[Text], classOf[Text]) hFile.collect 使用自定义InputFormat InputFormat是MapReduce框架下将输入的文件解析成字符串的组件，Spark对HDFS中的文件实现自定义读写需要通过InputFormat的子类实现。下面只写简单的思路，具体的可以参考InputFormat和MapReduce相关资料。
InputFormat的修改可以参考TextInputFormat，继承FileInputFormat后，重载createRecordReader返回一个新的继承RecordReader的类，通过新的RecordReader读取数据返回键值对。
打包后注意上传时将jar包一起上传：
`./spark-shell &ndash;jars newInputFormat.jar
运行的代码和上面差不多，import相关的包后
val hFile = sc.newAPIHadoopFile(&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;, classOf[NewTextInputFormat], classOf[Text], classOf[Text]) 一些坑 序列化问题 在读取文件后使用first或者collect时，出现下面的错误
  ERROR scheduler.TaskSetManager: Task 0.0 in stage 2.0 (TID 18) had a not serializable result: org.apache.hadoop.io.IntWritable Serialization stack: - object not serializable (class: org.apache.hadoop.io.IntWritable, value: 35) - element of array (index: 0) - array (class [Lorg.'>
<meta name='theme-color' content='#cccccc'>

<meta property='og:title' content='Spark设置自定义的InputFormat读取HDFS文件 • A Notebook of Extendswind'>
<meta property='og:description' content='Spark提供了HDFS上一般的文件文件读取接口 sc.textFile()，但在某些情况下HDFS中需要存储自定义格式的文件，需要更加灵活的读取方式。
使用KeyValueTextInputFormat Hadoop的MapReduce框架下提供了一些InputFormat的实现，其中MapReduce2的接口(org.apache.hadoop.mapreduce下)与先前MapReduce1(org.apache.hadoop.mapred下)有区别，对应于newAPIHadoopFile函数。
使用KeyValueTextInputFormat的文件读取如下
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat import org.apache.hadoop.io.Text val hFile = sc.newAPIHadoopFile(&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;, classOf[KeyValueTextInputFormat], classOf[Text], classOf[Text]) hFile.collect 使用自定义InputFormat InputFormat是MapReduce框架下将输入的文件解析成字符串的组件，Spark对HDFS中的文件实现自定义读写需要通过InputFormat的子类实现。下面只写简单的思路，具体的可以参考InputFormat和MapReduce相关资料。
InputFormat的修改可以参考TextInputFormat，继承FileInputFormat后，重载createRecordReader返回一个新的继承RecordReader的类，通过新的RecordReader读取数据返回键值对。
打包后注意上传时将jar包一起上传：
`./spark-shell &ndash;jars newInputFormat.jar
运行的代码和上面差不多，import相关的包后
val hFile = sc.newAPIHadoopFile(&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;, classOf[NewTextInputFormat], classOf[Text], classOf[Text]) 一些坑 序列化问题 在读取文件后使用first或者collect时，出现下面的错误
  ERROR scheduler.TaskSetManager: Task 0.0 in stage 2.0 (TID 18) had a not serializable result: org.apache.hadoop.io.IntWritable Serialization stack: - object not serializable (class: org.apache.hadoop.io.IntWritable, value: 35) - element of array (index: 0) - array (class [Lorg.'>
<meta property='og:url' content='https://extendswind.top/posts/technical/problem_spark_reading_hdfs_serializable/'>
<meta property='og:site_name' content='A Notebook of Extendswind'>
<meta property='og:type' content='article'><meta property='article:section' content='posts'><meta property='article:tag' content='hadoop'><meta property='article:tag' content='spark'><meta property='article:published_time' content='2018-12-15T11:30:00&#43;08:00'/><meta property='article:modified_time' content='2018-12-15T11:30:00&#43;08:00'/><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.70.0" />

  <title>Spark设置自定义的InputFormat读取HDFS文件 • A Notebook of Extendswind</title>
  <link rel='canonical' href='https://extendswind.top/posts/technical/problem_spark_reading_hdfs_serializable/'>
  
  
  <link rel='icon' href='/images/wind.jpeg'>
<link rel='stylesheet' href='/assets/css/main.6a060eb7.css'><link rel='stylesheet' href='/css/custom.css'><style>
:root{--color-accent:#cccccc;}
</style>

  

</head>
<body class='page type-posts has-sidebar'>

  <div class='site'><div id='sidebar' class='sidebar'>
  <a class='screen-reader-text' href='#main-menu'>Skip to Main Menu</a>

  <div class='container'><section class='widget widget-about sep-after'>
  <header>
    
    <div class='logo'>
      <a href='/'>
        <img src='/images/wind.jpeg'>
      </a>
    </div>
    
    <h2 class='title site-title '>
      <a href='/'>
      extendswind
      </a>
    </h2>
    <div class='desc'>
    
    </div>
  </header>

</section>
<section class='widget widget-sidebar_menu sep-after'><nav id='sidebar-menu' class='menu sidebar-menu' aria-label='Sidebar Menu'>
    <div class='container'>
      <ul><li class='item'>
  <a href='/'>Home</a></li><li class='item'>
  <a href='/posts'>Posts</a></li><li class='item'>
  <a href='/categories'>Categories</a></li><li class='item'>
  <a href='https://github.com/extendswind'>GitHub</a></li><li class='item'>
  <a href='https://www.cnblogs.com/fly2wind/'>Cnblog</a></li></ul>
    </div>
  </nav>

</section><section class='widget widget-taxonomy_cloud sep-after'>
  <header>
    <h4 class='title widget-title'>Tags</h4>
  </header>

  <div class='container list-container'>
  <ul class='list taxonomy-cloud'><li>
        <a href='/tags/blog/' style='font-size:1em'>blog</a>
      </li><li>
        <a href='/tags/design-patterns/' style='font-size:1em'>design patterns</a>
      </li><li>
        <a href='/tags/gis/' style='font-size:1em'>GIS</a>
      </li><li>
        <a href='/tags/git/' style='font-size:1em'>git</a>
      </li><li>
        <a href='/tags/hadoop/' style='font-size:1em'>hadoop</a>
      </li><li>
        <a href='/tags/java/' style='font-size:1em'>java</a>
      </li><li>
        <a href='/tags/life/' style='font-size:1em'>life</a>
      </li><li>
        <a href='/tags/linux/' style='font-size:1em'>linux</a>
      </li><li>
        <a href='/tags/log4j/' style='font-size:1em'>log4j</a>
      </li><li>
        <a href='/tags/raspberry/' style='font-size:1em'>raspberry</a>
      </li><li>
        <a href='/tags/reading/' style='font-size:1em'>reading</a>
      </li><li>
        <a href='/tags/software/' style='font-size:1em'>software</a>
      </li><li>
        <a href='/tags/spark/' style='font-size:1em'>spark</a>
      </li></ul>
</div>


</section>
</div>

  <div class='sidebar-overlay'></div>
</div><div class='main'><a class='screen-reader-text' href='#content'>Skip to Content</a>

<button id='sidebar-toggler' class='sidebar-toggler' aria-controls='sidebar'>
  <span class='screen-reader-text'>Toggle Sidebar</span>
  <span class='open'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="3" y1="12" x2="21" y2="12" />
  <line x1="3" y1="6" x2="21" y2="6" />
  <line x1="3" y1="18" x2="21" y2="18" />
  
</svg>
</span>
  <span class='close'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
  
</svg>
</span>
</button><div class='header-widgets'>
        <div class='container'></div>
      </div>

      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>A Notebook of Extendswind</p><p class='desc site-desc'>a simple blog site</p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='en' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Spark设置自定义的InputFormat读取HDFS文件</h1>
      

    </div>
    <div class='entry-meta'>
  <span class='posted-on'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>
<span class='screen-reader-text'>Posted on </span>
  <time class='entry-date' datetime='2018-12-15T11:30:00&#43;08:00'>2018, Dec 15</time>
</span>

  
  

</div>


  </div>
</header>

  
  
<details class='container entry-toc' open>
  <summary class='title'>
    <span>Table of Contents</span>
  </summary>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#使用keyvaluetextinputformat">使用KeyValueTextInputFormat</a></li>
    <li><a href="#使用自定义inputformat">使用自定义InputFormat</a></li>
    <li><a href="#一些坑">一些坑</a>
      <ul>
        <li><a href="#序列化问题">序列化问题</a></li>
        <li><a href="#javalangillegalstateexception-unread-block-data">java.lang.IllegalStateException: unread block data</a></li>
      </ul>
    </li>
  </ul>
</nav>
</details>


  <div class='container entry-content'>
  <p>Spark提供了HDFS上一般的文件文件读取接口 sc.textFile()，但在某些情况下HDFS中需要存储自定义格式的文件，需要更加灵活的读取方式。</p>
<h1 id="使用keyvaluetextinputformat">使用KeyValueTextInputFormat</h1>
<p>Hadoop的MapReduce框架下提供了一些InputFormat的实现，其中MapReduce2的接口(org.apache.hadoop.mapreduce下)与先前MapReduce1(org.apache.hadoop.mapred下)有区别，对应于newAPIHadoopFile函数。</p>
<p>使用KeyValueTextInputFormat的文件读取如下</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">
<span style="color:#66d9ef">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat
<span style="color:#66d9ef">import</span> org.apache.hadoop.io.Text

<span style="color:#66d9ef">val</span> hFile <span style="color:#66d9ef">=</span> sc<span style="color:#f92672">.</span>newAPIHadoopFile<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;</span><span style="color:#f92672">,</span>
    classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">KeyValueTextInputFormat</span><span style="color:#f92672">],</span> classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">Text</span><span style="color:#f92672">],</span> classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">Text</span><span style="color:#f92672">])</span>
    
hFile<span style="color:#f92672">.</span>collect

</code></pre></div><h1 id="使用自定义inputformat">使用自定义InputFormat</h1>
<p>InputFormat是MapReduce框架下将输入的文件解析成字符串的组件，Spark对HDFS中的文件实现自定义读写需要通过InputFormat的子类实现。下面只写简单的思路，具体的可以参考InputFormat和MapReduce相关资料。</p>
<p>InputFormat的修改可以参考TextInputFormat，继承FileInputFormat后，重载createRecordReader返回一个新的继承RecordReader的类，通过新的RecordReader读取数据返回键值对。</p>
<p>打包后注意上传时将jar包一起上传：</p>
<p>`./spark-shell &ndash;jars newInputFormat.jar</p>
<p>运行的代码和上面差不多，import相关的包后</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#66d9ef">val</span> hFile <span style="color:#66d9ef">=</span> sc<span style="color:#f92672">.</span>newAPIHadoopFile<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hdfs://hadoopmaster:9000/user/sparkl/README.md&#34;</span><span style="color:#f92672">,</span> 
    classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">NewTextInputFormat</span><span style="color:#f92672">],</span> classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">Text</span><span style="color:#f92672">],</span> classOf<span style="color:#f92672">[</span><span style="color:#66d9ef">Text</span><span style="color:#f92672">])</span>
</code></pre></div><h1 id="一些坑">一些坑</h1>
<h2 id="序列化问题">序列化问题</h2>
<p>在读取文件后使用first或者collect时，出现下面的错误</p>
<blockquote>
</blockquote>
<p>ERROR scheduler.TaskSetManager: Task 0.0 in stage 2.0 (TID 18) had a not serializable result: org.apache.hadoop.io.IntWritable
Serialization stack:
- object not serializable (class: org.apache.hadoop.io.IntWritable, value: 35)
- element of array (index: 0)
- array (class [Lorg.apache.hadoop.io.IntWritable;, size 1); not retrying
18/12/15 10:40:10 ERROR scheduler.TaskSetManager: Task 2.0 in stage 2.0 (TID 21) had a not serializable result: org.apache.hadoop.io.IntWritable
Serialization stack:
- object not serializable (class: org.apache.hadoop.io.IntWritable, value: 35)
- element of array (index: 0)
- array (class [Lorg.apache.hadoop.io.IntWritable;, size 1); not retrying</p>
<p>当键值对是其它的类型时，还可能出现类似的：</p>
<blockquote>
</blockquote>
<p>ERROR scheduler.TaskSetManager: Task 0.0 in stage 2.0 (TID 18) had a not serializable result: org.apache.hadoop.io.LongWritable
ERROR scheduler.TaskSetManager: Task 0.0 in stage 2.0 (TID 18) had a not serializable result: org.apache.hadoop.io.Text</p>
<p>此问题略奇怪，都实现了Hadoop的Writable接口，却不能被序列化。某些地方提到Hadoop与Spark没有使用同一套序列化机制，需要在Spark的序列化框架下注册才能使用。</p>
<p>一般更建议在drive程序上收集信息时，首先转换成基本的数据类型：</p>
<p>hFile.filter(k =&gt; k._1.toString.contains(&ldquo;a&rdquo;)).collect</p>
<h2 id="javalangillegalstateexception-unread-block-data">java.lang.IllegalStateException: unread block data</h2>
<blockquote>
</blockquote>
<p>ERROR executor.Executor: Exception in task 0.3 in stage 0.0 (TID 3)
java.lang.IllegalStateException: unread block data
at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2781)</p>
<p>一个很坑的错误，spark-shell下只出现这个，并未表明真正的错误在哪。在spark的webUI上能够看到相关的运行日志，上面的异常前还有一个异常写的是我重写的InputSplit没有实现Writable接口。<strong>此处的坑，InputFormat中用的InputSplit如果需要重写需要实现Writable接口，在MapReduce下使用貌似没有这一要求。</strong></p>
<p>补上之后上传到集群的nodemanager即可。<strong>注意，当nodemanager和spark-shell上传的jar包中有相同的类时，nodemanager优先使用了自身的类。</strong></p>

</div>

  
<footer class='entry-footer'>
  <div class='container sep-before'><div class='categories'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M22,19a2,2,0,0,1-2,2H4a2,2,0,0,1-2-2V5A2,2,0,0,1,4,3H9l2,3h9a2,2,0,0,1,2,2Z"/>
  
</svg>
<span class='screen-reader-text'>Categories: </span><a class='category' href='/categories/cloud-computing/'>cloud computing</a></div>
<div class='tags'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
  
</svg>
<span class='screen-reader-text'>Tags: </span><a class='tag' href='/tags/hadoop/'>hadoop</a>, <a class='tag' href='/tags/spark/'>spark</a></div>

  </div>
</footer>


</article>

<nav class='entry-nav'>
  <div class='container'><div class='prev-entry sep-before'>
      <a href='/posts/technical/hadoop_rack_awareness/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 Previous</span>
        <span class='screen-reader-text'>Previous post: </span>Hadoop 机架（集群拓扑）设置</a>
    </div><div class='next-entry sep-before'>
      <a href='/posts/research/zotero_multiple_directory_pdf_sync/'>
        <span class='screen-reader-text'>Next post: </span>zotero zotfile插件 pdf附件文件夹在多系统下的同步设置<span aria-hidden='true'>Next <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="4" y1="12" x2="20" y2="12"/>
  <polyline points="14 6 20 12 14 18"/>
  
</svg>
</span>
      </a>
    </div></div>
</nav>




      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'><section class='widget widget-social_menu sep-after'><header>
    <h4 class='title widget-title'>Contact</h4>
  </header><nav aria-label='Social Menu'>
    <ul><li>
        <a href='https://github.com/extendswind' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Open Github account in new tab</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
  
</svg>
</a>
      </li><li>
        <a href='mailto:extendswind@foxmail.com' target='_blank' rel='noopener'>
          <span class='screen-reader-text'>Contact via Email</span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
  <polyline points="22,6 12,13 2,6"/>
  
</svg>
</a>
      </li></ul>
  </nav>
</section><div class='copyright'>
  <p> &copy; 2018-2020 extendswind </p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__assets_js_src="/assets/js/"</script>

<script src='/assets/js/main.67d669ac.js'></script>

</body>

</html>

